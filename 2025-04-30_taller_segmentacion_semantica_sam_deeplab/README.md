# ğŸ§ª Taller - SegmentaciÃ³n SemÃ¡ntica Multimodal: QuÃ© hay en la Imagen

## ğŸ“… Fecha
`2025-04-30` â€“ Fecha de realizaciÃ³n

---

## ğŸ¯ Objetivo del Taller

Aplicar segmentaciÃ³n semÃ¡ntica utilizando modelos avanzados (SAM, DeepLab) para identificar y extraer regiones especÃ­ficas (personas, Ã¡rboles, vehÃ­culos, etc.) dentro de imÃ¡genes.

Obtener las mÃ¡scaras de estas regiones para su posterior anÃ¡lisis, recorte o visualizaciÃ³n.

---

## ğŸ§  Conceptos Aprendidos

Lista los principales conceptos aplicados:

- [x] SegmentaciÃ³n de imÃ¡genes
- [x] Modelos de SegmentaciÃ³n (SAM)
- [x] ObtenciÃ³n y uso de mÃ¡scaras
- [x] AnÃ¡lisis y visualizaciÃ³n de resultados de segmentaciÃ³n

---

## ğŸ”§ Herramientas y Entornos

Especifica los entornos usados:

- [x] Python (Google Colab o local)
  - LibrerÃ­as clave: `opencv-python`, `torch`, `numpy`, `matplotlib`, `Pillow`, `segment-anything`, `supervision`, `transformers`, `huggingface_hub`
- [x] Jupyter / Google Colab


---

## ğŸ“ Estructura del Proyecto

```
2025-04-30_taller_segmentacion_semantica_sam_deeplab/
â”œâ”€â”€ ColabNotebooks/               
â”‚   â””â”€â”€ sam_segmentation.ipynb
â”‚   â””â”€â”€ sam_vit_h_4b8939.pth
â”œâ”€â”€ imagenes_entrada/
â”‚   â””â”€â”€ imagen.jpg
â”œâ”€â”€ mascaras_salida/               
â”‚   â””â”€â”€ mask_0.png
â”‚   â””â”€â”€ mask_1.png
â”‚   â””â”€â”€ mask_2.png
â”œâ”€â”€ resultados/               
â”‚   â””â”€â”€ imagen_original.png
â”‚   â””â”€â”€ mask_colored_0.png
â”‚   â””â”€â”€ mask_colored_1.png
â”‚   â””â”€â”€ mask_colored_2.png
â”‚   â””â”€â”€ segmentation_process.gif
â”œâ”€â”€ README.md
```

---

## ğŸ“– DescripciÃ³n del Modelo Usado: SAM (Segment Anything Model)

SAM, desarrollado por Meta AI, es un modelo de segmentaciÃ³n de imÃ¡genes de vanguardia diseÃ±ado para ser un "modelo fundacional" en este campo. Su principal caracterÃ­stica es su capacidad para segmentar cualquier objeto en una imagen, incluso aquellos que no ha visto durante su entrenamiento.

A diferencia de modelos de segmentaciÃ³n tradicionales que estÃ¡n entrenados para reconocer un conjunto fijo de categorÃ­as (como "persona", "coche", "Ã¡rbol"), SAM puede generar mÃ¡scaras de segmentaciÃ³n de alta calidad para objetos basÃ¡ndose en diferentes tipos de "prompts" o indicaciones, como un simple clic en el objeto, un recuadro que lo delimite, o incluso texto (aunque la implementaciÃ³n en el notebook se basa en puntos o cajas). Esto lo hace increÃ­blemente flexible y Ãºtil para identificar y extraer regiones arbitrarias en una imagen.

En resumen, SAM nos permite obtener mÃ¡scaras precisas de los elementos que nos interesan en una imagen de forma muy efectiva.

---

## ğŸ§ª ImplementaciÃ³n

Antes de explicar la implementaciÃ³n, es importante recalcar que el desarrollo se realizÃ³ en Colab y el notebook del repositorio es meramente ilustrativo. Para ejecutar el proyecto, consultar el notebook en: [sam_segmentation.ipynb](https://colab.research.google.com/drive/1R_N0eKghF6xmVljKvBXjIdLsyHWx8fn8?usp=sharing)

El proceso llevado a cabo para la segmentaciÃ³n semÃ¡ntica de imÃ¡genes fue el siguiente:

### ğŸ”¹ Etapas realizadas

Describe brevemente cada fase del proceso:

1.  **PreparaciÃ³n de la imagen:** Carga de la imagen de entrada y su preprocesamiento necesario segÃºn el modelo a utilizar (ajuste de tamaÃ±o, normalizaciÃ³n de valores de pÃ­xeles, etc.). Para SAM, esto incluye "setear" la imagen en el predictor.
2.  **AplicaciÃ³n del modelo de segmentaciÃ³n:** Pasar la imagen preprocesada a travÃ©s del modelo de IA elegido (SAM o DeepLab) para generar las predicciones de segmentaciÃ³n a nivel de pÃ­xel.
3.  **ExtracciÃ³n de mÃ¡scaras:** Procesar la salida del modelo para obtener las mÃ¡scaras binarias o segmentadas que corresponden a las regiones de interÃ©s identificadas (personas, objetos, etc.).
4.  **VisualizaciÃ³n de resultados:** Mostrar la imagen original junto con las mÃ¡scaras generadas o la imagen segmentada para verificar los resultados.
5.  **Guardado de resultados:** Almacenar las mÃ¡scaras generadas y/o las visualizaciones finales para su uso posterior.

### ğŸ”¹ CÃ³digo relevante

Incluye un fragmento que resuma el corazÃ³n del taller, mostrando cÃ³mo se obtiene la predicciÃ³n principal del modelo:

```python
# Aplicar el modelo a la imagen preprocesada y obtener la predicciÃ³n
# (Este ejemplo es especÃ­fico para la salida tÃ­pica de DeepLab)
output = model(input_tensor)['out'] # Obtiene el diccionario de salida y selecciona la clave 'out' con las predicciones logit
prediction = output.argmax(1).squeeze().cpu().numpy() # Encuentra la clase con mayor puntuaciÃ³n para cada pÃ­xel, elimina dimensiones extra y convierte a array de NumPy en CPU
```

---

## ğŸ“Š Resultados Visuales

### ğŸ“Œ Este taller **requiere explÃ­citamente un GIF animado** y las mascaras obtenidas:

## Secuencia del proceso de segmentaciÃ³n

![proceso de segmentacion](resultados/segmentation_process.gif)

## Imagen original

![imagen original](resultados/imagen_original.png)

Es un gato muy bonito porque, Â¿QuiÃ©n no ama los gatos? ğŸˆ

## MÃ¡scara 0

![mascara 0](resultados/mask_colored_0.png)

* Segmenta parcialmente al gato, especialmente el torso.

* **Problemas:** la textura del fondo (escaleras) interfiere y provoca pÃ©rdida de detalle, dejando Ã¡reas internas sin segmentar (agujeros).

* **Probable entrada:** un punto cerca del cuerpo, pero sin ayudar a delimitar con claridad.

## MÃ¡scara 1

![mascara 1](resultados/mask_colored_1.png)

* SegmentaciÃ³n mÃ¡s precisa del gato completo, incluyendo cabeza, cuerpo y cola.

* Contorno limpio, sin mucha fuga hacia las escaleras.

* Esta es claramente la mÃ¡s Ãºtil de las tres para propÃ³sitos de segmentaciÃ³n del objeto principal.

*  **Probable entrada:** punto centrado en el cuerpo o mÃºltiples entradas (punto + caja).

## MÃ¡scara 2

![mascara 2](resultados/mask_colored_2.png)

* Incluye al gato y parte del fondo (escalones).

* Tiene fugas significativas en la base, capturando sombras o estructuras del entorno.

* Es Ãºtil si quieres una mÃ¡scara mÃ¡s "contextual", pero no sirve para aislar solo al gato.

---

## ğŸ§© Prompts Usados

En este caso solo se solicito un cÃ³digo para la visualizaciÃ³n de las mÃ¡scaras obtenidas y que nos permitiera guardarlas como imagenes binarizadas.

```text
1. "Estoy usando SAM en Python. Genera un cÃ³digo que me permita visualizar los resultados de Segment Anything con colores distintos y guardar las mÃ¡scaras como imagenes binarizadas"
```
---

## ğŸ’¬ ReflexiÃ³n Final

Este taller me permitiÃ³ explorar el uso de modelos avanzados de segmentaciÃ³n semÃ¡ntica como Segment Anything Model (SAM) de Meta AI. A diferencia de tÃ©cnicas tradicionales, SAM permite segmentar regiones especÃ­ficas de una imagen con solo seleccionar un punto o una caja, lo cual resulta muy Ãºtil en entornos interactivos o donde se requiere precisiÃ³n.

AprendÃ­ a instalar el modelo, cargar imÃ¡genes, generar mÃ¡scaras y visualizarlas correctamente. Me pareciÃ³ interesante cÃ³mo SAM generaliza bien incluso en imÃ¡genes nuevas sin necesidad de reentrenamiento. AdemÃ¡s, comprender cÃ³mo se representan las mÃ¡scaras y cÃ³mo exportarlas para su anÃ¡lisis posterior me ayudÃ³ a conectar esta tÃ©cnica con aplicaciones reales de visiÃ³n por computador.

En general, este taller me mostrÃ³ cÃ³mo el estado del arte en segmentaciÃ³n ya estÃ¡ disponible para ser usado directamente en proyectos personales o profesionales, y me motiva a seguir explorando otras tÃ©cnicas basadas en IA multimodal.

---

## âœ… Checklist de Entrega


 - [x] Carpeta `2025-04-30_taller_segmentacion_semantica_sam_deeplab`
 - [x] CÃ³digo funcional en Colab o Jupyter Notebook (.ipynb)
 - [x] Imagen original utilizada (imagenes_entrada/)
 - [x] MÃ¡scaras generadas en formato binario (mascaras_salida/)
 - [x] VisualizaciÃ³n de resultados con anotaciones (resultados/)
 - [x] GIF animado que muestre el proceso de segmentaciÃ³n
 - [x] README completo con secciones: objetivo, actividades, prompts, reflexiÃ³n, evidencias
 - [x] Commits realizados con mensajes descriptivos en inglÃ©s

---